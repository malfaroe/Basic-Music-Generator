{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7316,"status":"ok","timestamp":1646675167093,"user":{"displayName":"Mauricio Alfaro","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16047939764221462828"},"user_tz":180},"id":"7k90mci76m8o"},"outputs":[],"source":["\"\"\"Unit for training LSTM engines intended for music generation projects\n","We will be using the Drive directory called maeGenerator22\n","\n","\n","Blocks:\n","\n","1. Loading the training data and parameters: \n","X: raw input data\n","X_f: input data already normalized with MinMax and reshaped for the\n","LSTM with the shape: (len(X_f), SEQUENCE_LENGTH,1)\n","Where SEQUENCE_LENGTH is the number of elements of each training sequence\n","(or the len() of each data example)\n","vocab_length = number of classes in the data target for training\n","\n","These 3 elements will be imported from the previous preprocessing stage\n","called modular.py\n","\n","2. Training block\n","3. Saving the generated model for next stage\"\"\"\n","\n","#Checking GPU opertion\n","import tensorflow as tf\n","tf.test.gpu_device_name() #debe dar \"/device: GPU:0\" otherwise \"\"\n","\n","from re import S\n","import music21 as m21\n","import pandas as pd\n","import numpy as np\n","import os\n","import keras\n","#from keras.utils import to_categorical\n","from tensorflow.keras.utils import to_categorical\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","#For training unit\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.layers import Activation\n","from keras.layers import BatchNormalization as BatchNorm\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model\n","from keras.utils import np_utils\n","\n","import json\n","import glob\n","import pickle as pkl\n","\n","import logging\n","\n","logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) #for avoiding annoying tf warnings\n","\n","\n","\n","###LOAD THE DATA AND PARAMETERS\n","\n","data_path = \"/content/drive/MyDrive/maeGenerator22/training_data.pkl\"\n","\n","def data_load(data_path):\n","    \"\"\"Loads the training data and parameters using pickle\n","args: data_path: location of the pkl file containing the data\n","called training_data.pkl and unpacks all the data\n","\"\"\"\n","    with open(data_path, \"rb\") as f:\n","        X, X_f, y, vocab_length = pkl.load(f)\n","    print(\"Loaded data summary:\")\n","    print(\"=======================\")\n","    print(\"Number of training examples:\", len(X))\n","    print(\"Processed input data size:\", X_f.shape)\n","    print(\"Target data size:\", y.shape)\n","    print(\"Nr of target classes:\", vocab_length)\n","    return X, X_f, y, vocab_length\n","\n","\n","#####TRAINING UNIT\n","OUTPUT_UNITS = None #to be obtained as vocab_size variable from generate_training_sequences\n","NUM_UNITS = [256] #Hidden layer units\n","LOSS = \"sparse_categorical_crossentropy\"\n","LEARNING_RATE = 0.001\n","EPOCHS = 50\n","BATCH_SIZE = 128\n","SAVED_MODEL_NAME = \"LSTM_MODEL.h5\"\n","\n","\n","def train_model(model, inputs, targets, model_name = SAVED_MODEL_NAME,\n"," batch_size = BATCH_SIZE, epochs = EPOCHS):\n","    \"\"\"Train and save model\"\"\"\n","    filepath = \"/content/drive/MyDrive/maeGenerator22/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n","    checkpoint = ModelCheckpoint(\n","        filepath,\n","        monitor='loss',\n","        verbose=0,\n","        save_best_only=True,\n","        mode='min'\n","    )\n","    callbacks_list = [checkpoint]\n","   \n","    model.fit(inputs, targets,\n"," batch_size = BATCH_SIZE, epochs = epochs, callbacks= callbacks_list)\n","\n","    #Save the model\n","    model.save(model_name)\n","    print(\"Training complete!\")\n","\n","    return model\n","\n","def build_the_model(inputs, vocab_size):\n","    \"\"\"Create the architecture of the network\"\"\"\n","    model = Sequential()\n","    model.add(LSTM(512, \n","    input_shape = (inputs.shape[1], inputs.shape[2]),\n","    recurrent_dropout= 0.3,\n","    return_sequences= True))\n","\n","    model.add(LSTM(512,recurrent_dropout= 0.3, return_sequences= True))\n","    model.add(LSTM(512))\n","    model.add(BatchNorm())\n","    model.add(Dropout(0.3))\n","    model.add(Dense(256))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNorm())\n","    model.add(Dropout(0.3))\n","    model.add(Dense(vocab_size))\n","    model.add(Activation(\"softmax\"))\n","    \n","    model.compile(loss =\"categorical_crossentropy\", optimizer = \"rmsprop\")\n","    #model.summary()\n","    return model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"RZ_UGUYqBak1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded data summary:\n","=======================\n","Number of training examples: 209471\n","Processed input data size: (209471, 100, 1)\n","Target data size: (209471, 445)\n","Nr of target classes: 445\n","Epoch 1/30\n","1637/1637 [==============================] - 1748s 1s/step - loss: 1.1528\n","Epoch 2/30\n","1637/1637 [==============================] - 1729s 1s/step - loss: 1.1493\n","Epoch 3/30\n","1637/1637 [==============================] - 1737s 1s/step - loss: 1.1475\n","Epoch 4/30\n","1637/1637 [==============================] - 1751s 1s/step - loss: 1.1443\n","Epoch 5/30\n","1637/1637 [==============================] - 1747s 1s/step - loss: 1.1385\n","Epoch 6/30\n","1637/1637 [==============================] - 1732s 1s/step - loss: 1.1286\n","Epoch 7/30\n"," 337/1637 [=====\u003e........................] - ETA: 22:09 - loss: 1.0745"]}],"source":["#TESTING\n","\n","if __name__ == \"__main__\":\n","    #ORIGINAL SEQUENCE\n","    #   X, X_f, y, vocab_length = data_load(data_path)\n","    #   model = build_the_model(X_f, vocab_length)\n","    #   model = train_model(model, inputs = X_f, targets = y, model_name = SAVED_MODEL_NAME,batch_size = BATCH_SIZE, epochs = EPOCHS)\n","#     #prediction = generate_notes(X, SAVED_MODEL_NAME, list(set(notes)), vocab_length, 0.9)\n","#     #convert_to_midi(prediction)\n","#     print(\"Done!\")\n","\n","    #LOADING A CHECKPOINT AND CONTINUE\n","    TEST_EPOCHS = 30 #Test for 2 epochs\n","    X, X_f, y, vocab_length = data_load(data_path)\n","    new_model = load_model(\"/content/drive/MyDrive/maeGenerator22/weights-7PM.hdf5\")\n","    model = train_model(new_model, inputs = X_f, targets = y, model_name = SAVED_MODEL_NAME,batch_size = BATCH_SIZE, epochs = TEST_EPOCHS)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbR-yZG2DfWl"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNzqgNoH+87wOUaEG5ka0lA","collapsed_sections":[],"mount_file_id":"1sCTqyJSEgsfS6_9XfJhr9nNprekC0oJX","name":"LSTMtrain.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}